{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Prj_CreditCardFraud.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObpWEDnL3Fow",
        "colab_type": "text"
      },
      "source": [
        "Load Google Drive CSV into Pandas DataFrame for Google Colaboratory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYYoRTmE3Mgz",
        "colab_type": "text"
      },
      "source": [
        "Step 1 : Run the given below code Step 1, You will receive a prompt to enter the verification Code. The Verification code will get if you click the blue color link above the verification code. Copy the verification code after you enter the gmail login id\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4T4BGJpHzdSo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Code for Step 1\n",
        "!pip install -U -q PyDrive\n",
        " \n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        " \n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wlo9Va2v3jeG",
        "colab_type": "text"
      },
      "source": [
        "Step 2: provide the name of the dataset name(example:creditcard.csv) and also id ( you will get the Id if you open the google dirve folder where the file resides)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhLI_wr61dV_",
        "colab_type": "code",
        "outputId": "6bca7eb8-6d4f-42b9-ccf3-91f7e42237b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "listed = drive.ListFile({'q': \"title contains 'creditcard.csv' and '1eJpQaJ8pwwqpCgdA6spl3dCANpB5MUYV' in parents\"}).GetList()\n",
        "\n",
        "for file in listed:\n",
        "  print('title {}, id {}'.format(file['title'], file['id']))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "title creditcard.csv, id 1DmF6rdOWVGrEWBctG6wMq0PhLH49bVbR\n",
            "title creditcard.csv.zip, id 1QmpWo8Msdsihpu3iw4LcL-3JpCv0Fez6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4XeIWH23xBA",
        "colab_type": "text"
      },
      "source": [
        "Step 3 : Pandas can;t read the google drive file directly , Hence you require to map the google drive file to a google script. After Step2 Execution you will recive and id for the file. Copy the id and paste in the given below first line code \n",
        "Example : drive.CreateFile({'id':'..id here....'})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNboHhc_AckS",
        "colab_type": "text"
      },
      "source": [
        "Most of the transactions were Non-Fraud (99.83%) of the time, while Fraud transactions occurs (017%) of the time in the dataframe. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_cxyo9l2SR5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_downloaded = drive.CreateFile({'id': '1DmF6rdOWVGrEWBctG6wMq0PhLH49bVbR'})\n",
        "train_downloaded.GetContentFile('creditfraud.csv')\n",
        "#test_downloaded = drive.CreateFile({'id': '<TEST_FILE_ID>'})\n",
        "#test_downloaded.GetContentFile('test.csv')  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_Re2APvAmrN",
        "colab_type": "text"
      },
      "source": [
        "Introduction\n",
        "\n",
        "---\n",
        "\n",
        "Oftentimes in practical machine learning problems there will be significant differences in the rarity of different classes of data being predicted. For example, when detecting cancer we can expect to have datasets with large numbers of false outcomes, and a relatively smaller number of true outcomes.\n",
        "\n",
        "The overall performance of any model trained on such data will be constrained by its ability to predict rare points. In problems where these rare points are only equally important or perhaps less important than non-rare points, this constraint may only become significant in the later \"tuning\" stages of building the model. But in problems where the rare points are important, or even the point of the classifier (as in a cancer example), dealing with their scarcity is a first-order concern for the mode builder.\n",
        "\n",
        "Tangentially, note that the relative importance of performance on rare observations should inform your choice of error metric for the problem you are working on; the more important they are, the more your metric should penalize underperformance on them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lO0H1r79BUFs",
        "colab_type": "text"
      },
      "source": [
        "Below is the Example for Oversampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bc47rXVSBLMF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import LinearSVC\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "def create_dataset(n_samples=1000, weights=(0.01, 0.01, 0.98), n_classes=3,\n",
        "                   class_sep=0.8, n_clusters=1):\n",
        "    return make_classification(n_samples=n_samples, n_features=2,\n",
        "                               n_informative=2, n_redundant=0, n_repeated=0,\n",
        "                               n_classes=n_classes,\n",
        "                               n_clusters_per_class=n_clusters,\n",
        "                               weights=list(weights),\n",
        "                               class_sep=class_sep, random_state=0)\n",
        "\n",
        "def plot_decision_function(X, y, clf, ax):\n",
        "    plot_step = 0.02\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
        "                         np.arange(y_min, y_max, plot_step))\n",
        "\n",
        "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    ax.contourf(xx, yy, Z, alpha=0.4)\n",
        "    ax.scatter(X[:, 0], X[:, 1], alpha=0.8, c=y, edgecolor='k')\n",
        "    \n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "ax_arr = (ax1, ax2, ax3, ax4)\n",
        "weights_arr = ((0.01, 0.01, 0.98), (0.01, 0.05, 0.94),\n",
        "               (0.2, 0.1, 0.7), (0.33, 0.33, 0.33))\n",
        "for ax, weights in zip(ax_arr, weights_arr):\n",
        "    X, y = create_dataset(n_samples=1000, weights=weights)\n",
        "    clf = LinearSVC().fit(X, y)\n",
        "    plot_decision_function(X, y, clf, ax)\n",
        "    ax.set_title('Linear SVC with y={}'.format(Counter(y)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ee4Mg5DA1Yx",
        "colab_type": "text"
      },
      "source": [
        "Several different techniques exist in the practice for dealing with imbalanced dataset. The most naive class of techniques is sampling: changing the data presented to the model by undersampling common classes, oversampling (duplicating) rare classes, or both."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IgqPYCvBhbc",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "As you can see, when a dataset is dominated by one or few classes, to the exclusion of some other classes, the optimal solution can break down to collapse: a model which simply classifies all or most points in the majority class (as in the first and second visualizations in this grid). However, as the number of observations per point approaches an equal split, the classifier becomes less and less biased.\n",
        "\n",
        "Re-sampling points that are being fed into the model is the simplest way to fix model errors like this one stemming from rare class problems. Not all datasets have this issue, but for those that due, dealing with this issue is an important early step in modeling the data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S45O1ItIRDXh",
        "colab_type": "text"
      },
      "source": [
        "**PROJECT ASSIGNMENT**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gv6W8VrdBjkc",
        "colab_type": "text"
      },
      "source": [
        "**Credit Card Fraud Detection**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R15ryCk0DTdP",
        "colab_type": "text"
      },
      "source": [
        "It is important that credit card companies are able to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmQL76ilDfFW",
        "colab_type": "text"
      },
      "source": [
        "V1-V28 Column Features result of a PCA Dimensionality reduction to protect user identities and sensitive features(v1-v28)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0G2f66PDl4d",
        "colab_type": "text"
      },
      "source": [
        "We have 31 Columns where the 30 Columns are Feature Representation and 1 Column is for Class Distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMgeQF5p_lEJ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "1.   Time :Number of seconds elapsed between this transaction and the first transaction in the dataset\n",
        "\n",
        "2.   Amount:Transaction amount\n",
        "\n",
        "*    Class : 1 Fraud transactions ,0 Non Fraud transactions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHcMZZV9DA1X",
        "colab_type": "text"
      },
      "source": [
        "For detecting Fraud in Credit card we shall check using the below classification Algorithms Logistic Regression Na√Øve Bayes Stochastic Gradient Descent K-Nearest Neighbours Decision Tree Random Forest Support Vector Machine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkFiCk4METtO",
        "colab_type": "text"
      },
      "source": [
        "**Define the Libraries for the projct**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EXtdffBEb0P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Code here\n",
        "                #library for matplotlib\n",
        "                #library for train_test_split\n",
        "                #library for SMoTE\n",
        "                #library for Seaborn\n",
        "                #library for numpy\n",
        "                #library for standardscaler\n",
        "                #library for robustscaler\n",
        "                #library for StratifiedShuffleSplit\n",
        "                #library for Logistic Regressoin\n",
        "                #library for Naive Bayes\n",
        "                #library for KNN\n",
        "                #library for Decision Tree\n",
        "                #library for Random Forest\n",
        "                #library for Suppor Vector Machine (SVM)\n",
        "                #library for Confusion Matrix\n",
        "                "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3KRioigFBFe",
        "colab_type": "text"
      },
      "source": [
        "**To ignore the warnings , please use below code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTsjzmZnFGbn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import warnings filter\n",
        "from warnings import simplefilter\n",
        "# ignore all future warnings\n",
        "simplefilter(action='ignore', category=FutureWarning)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGGRi9RzENS3",
        "colab_type": "text"
      },
      "source": [
        "**Load the dataset file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYDvyH-I27UY",
        "colab_type": "code",
        "outputId": "14f0934a-b02f-4bb3-a66b-e30076e9d464",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "cc_fraud = pd.read_csv('creditfraud.csv')\n",
        "cc_fraud.head(3)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>V13</th>\n",
              "      <th>V14</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V17</th>\n",
              "      <th>V18</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.359807</td>\n",
              "      <td>-0.072781</td>\n",
              "      <td>2.536347</td>\n",
              "      <td>1.378155</td>\n",
              "      <td>-0.338321</td>\n",
              "      <td>0.462388</td>\n",
              "      <td>0.239599</td>\n",
              "      <td>0.098698</td>\n",
              "      <td>0.363787</td>\n",
              "      <td>0.090794</td>\n",
              "      <td>-0.551600</td>\n",
              "      <td>-0.617801</td>\n",
              "      <td>-0.991390</td>\n",
              "      <td>-0.311169</td>\n",
              "      <td>1.468177</td>\n",
              "      <td>-0.470401</td>\n",
              "      <td>0.207971</td>\n",
              "      <td>0.025791</td>\n",
              "      <td>0.403993</td>\n",
              "      <td>0.251412</td>\n",
              "      <td>-0.018307</td>\n",
              "      <td>0.277838</td>\n",
              "      <td>-0.110474</td>\n",
              "      <td>0.066928</td>\n",
              "      <td>0.128539</td>\n",
              "      <td>-0.189115</td>\n",
              "      <td>0.133558</td>\n",
              "      <td>-0.021053</td>\n",
              "      <td>149.62</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.191857</td>\n",
              "      <td>0.266151</td>\n",
              "      <td>0.166480</td>\n",
              "      <td>0.448154</td>\n",
              "      <td>0.060018</td>\n",
              "      <td>-0.082361</td>\n",
              "      <td>-0.078803</td>\n",
              "      <td>0.085102</td>\n",
              "      <td>-0.255425</td>\n",
              "      <td>-0.166974</td>\n",
              "      <td>1.612727</td>\n",
              "      <td>1.065235</td>\n",
              "      <td>0.489095</td>\n",
              "      <td>-0.143772</td>\n",
              "      <td>0.635558</td>\n",
              "      <td>0.463917</td>\n",
              "      <td>-0.114805</td>\n",
              "      <td>-0.183361</td>\n",
              "      <td>-0.145783</td>\n",
              "      <td>-0.069083</td>\n",
              "      <td>-0.225775</td>\n",
              "      <td>-0.638672</td>\n",
              "      <td>0.101288</td>\n",
              "      <td>-0.339846</td>\n",
              "      <td>0.167170</td>\n",
              "      <td>0.125895</td>\n",
              "      <td>-0.008983</td>\n",
              "      <td>0.014724</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.358354</td>\n",
              "      <td>-1.340163</td>\n",
              "      <td>1.773209</td>\n",
              "      <td>0.379780</td>\n",
              "      <td>-0.503198</td>\n",
              "      <td>1.800499</td>\n",
              "      <td>0.791461</td>\n",
              "      <td>0.247676</td>\n",
              "      <td>-1.514654</td>\n",
              "      <td>0.207643</td>\n",
              "      <td>0.624501</td>\n",
              "      <td>0.066084</td>\n",
              "      <td>0.717293</td>\n",
              "      <td>-0.165946</td>\n",
              "      <td>2.345865</td>\n",
              "      <td>-2.890083</td>\n",
              "      <td>1.109969</td>\n",
              "      <td>-0.121359</td>\n",
              "      <td>-2.261857</td>\n",
              "      <td>0.524980</td>\n",
              "      <td>0.247998</td>\n",
              "      <td>0.771679</td>\n",
              "      <td>0.909412</td>\n",
              "      <td>-0.689281</td>\n",
              "      <td>-0.327642</td>\n",
              "      <td>-0.139097</td>\n",
              "      <td>-0.055353</td>\n",
              "      <td>-0.059752</td>\n",
              "      <td>378.66</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Time        V1        V2        V3  ...       V27       V28  Amount  Class\n",
              "0   0.0 -1.359807 -0.072781  2.536347  ...  0.133558 -0.021053  149.62      0\n",
              "1   0.0  1.191857  0.266151  0.166480  ... -0.008983  0.014724    2.69      0\n",
              "2   1.0 -1.358354 -1.340163  1.773209  ... -0.055353 -0.059752  378.66      0\n",
              "\n",
              "[3 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsbQVnFXCuLl",
        "colab_type": "text"
      },
      "source": [
        "Display the Column Shape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ttb55ziHC4dc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Code Here\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyZ8U17MFWV0",
        "colab_type": "text"
      },
      "source": [
        "Display the columns from dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIyrW1JhFaMq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#code here\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eF_XV13HFbsq",
        "colab_type": "text"
      },
      "source": [
        "Verify the dataframe have null values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvADs-OLFhAw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#code here\n",
        "'''example\n",
        "cc_fraud.isnull().sum().max()'''\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4og2jtBFib4",
        "colab_type": "text"
      },
      "source": [
        "Verify the dataframe datatypes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoYc_n-9GD6Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQx_PM51GFVM",
        "colab_type": "text"
      },
      "source": [
        "Display the total number of values for each class (Example: How many values for Fraud and No-Fraud)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJd6QShEGS5u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERHNdL3NGSeA",
        "colab_type": "text"
      },
      "source": [
        "Display a Graph shows total count for Fraud Vs No-Fraud Using Matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHEq3jYmGixo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjYCLkadGn2p",
        "colab_type": "text"
      },
      "source": [
        "Display a Graph shows total count for Fraud Vs No-Fraud Using Seaborn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvs38yUnGsP_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmGsDBtvJjbF",
        "colab_type": "text"
      },
      "source": [
        "Show the Values in Percentage Fraud Vs No-Fraud"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ccauw9WaJm_y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Code Here\n",
        "\n",
        "'''Example:\n",
        "print('No Frauds', round(ccfraud['Class'].value_counts()[0]/len(ccfraud) * 100,2), '% of the dataset')\n",
        "print('Frauds', round(ccfraud['Class'].value_counts()[1]/len(ccfraud) * 100,2), '% of the dataset')\n",
        "'''\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mo7Dn8Y3J48H",
        "colab_type": "text"
      },
      "source": [
        "Notice how imbalanced is our original dataset! Most of the transactions are non-fraud. If we use this dataframe as the base for our predictive models and analysis we might get a lot of errors and our algorithms will probably overfit since it will \"assume\" that most transactions are not fraud. But we don't want our model to assume, we want our model to detect patterns that give signs of fraud!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeMr9v_NKt5Q",
        "colab_type": "text"
      },
      "source": [
        "Display the graph showing the Class Distribution using Seaborn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJGKXaLDKG1b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Code Here\n",
        "'''\n",
        "colors = [\"#0101DF\", \"#DF0101\"]\n",
        "\n",
        "sns.countplot('Class', data=cc_fraud, palette=colors)\n",
        "plt.title('Class Distributions \\n (0: No Fraud || 1: Fraud)', fontsize=14)\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlE78D-FHwKA",
        "colab_type": "text"
      },
      "source": [
        "Display the Histogram for the datafrmae CLASS column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5y42iYXpH1Un",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#code here\n",
        "'''example:\n",
        "x=data['Class']\n",
        "bins = 2\n",
        "plt.hist(x,bins)\n",
        "plt.show()\n",
        "'''\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWpQkdlsLBAO",
        "colab_type": "text"
      },
      "source": [
        "**Distributions**: \n",
        "By seeing the distributions we can have an idea how skewed are these features, we can also see further distributions of the other features. There are techniques that can help the distributions be less skewed which will be implemented in this notebook in the future."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89qnvvhgLZUS",
        "colab_type": "text"
      },
      "source": [
        "Define the Distributions based on the Time and Amount columns from the Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Aw1st7MLHCI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Code here\n",
        "\n",
        "'''\n",
        "fig, ax = plt.subplots(1, 2, figsize=(18,4))\n",
        "\n",
        "amount_val = cc_fraud['Amount'].values\n",
        "time_val = cc_fraud['Time'].values\n",
        "\n",
        "sns.distplot(amount_val, ax=ax[0], color='r')\n",
        "ax[0].set_title('Distribution of Transaction Amount', fontsize=14)\n",
        "ax[0].set_xlim([min(amount_val), max(amount_val)])\n",
        "\n",
        "sns.distplot(time_val, ax=ax[1], color='b')\n",
        "ax[1].set_title('Distribution of Transaction Time', fontsize=14)\n",
        "ax[1].set_xlim([min(time_val), max(time_val)])\n",
        "plt.show()\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heuDC9KyIm52",
        "colab_type": "text"
      },
      "source": [
        "**Scaling and Distributing**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "In this phase, we will first scale the columns comprise of Time and Amount . Time and amount should be scaled as the other columns. On the other hand, we need to also create a sub sample of the dataframe in order to have an equal amount of Fraud and Non-Fraud cases, helping our algorithms better understand patterns that determines whether a transaction is a fraud or not.\n",
        "\n",
        "What is a sub-Sample?\n",
        "In this scenario, our subsample will be a dataframe with a 50/50 ratio of fraud and non-fraud transactions. Meaning our sub-sample will have the same amount of fraud and non fraud transactions.\n",
        "\n",
        "Why do we create a sub-Sample?\n",
        "In the beginning of this notebook we saw that the original dataframe was heavily imbalanced! Using the original dataframe will cause the following issues:\n",
        "\n",
        "\n",
        "1.   Overfitting: Our classification models will assume that in most cases there are no frauds! What we want for our model is to be certain when a fraud occurs.\n",
        "2.   Wrong Correlations: Although we don't know what the \"V\" features stand for, it will be useful to understand how each of this features influence the result (Fraud or No Fraud) by having an imbalance dataframe we are not able to see the true correlations between the class and features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmWeXoMxIYQF",
        "colab_type": "text"
      },
      "source": [
        "Data is highly Unbalanced.Amount column needs to be rescaled after which we can drop Amount and time column\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csl4xQGlMAJ6",
        "colab_type": "text"
      },
      "source": [
        "Mosf of the Featurs have Scaled except the Time and Amount Columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuI8Rj_JMF1-",
        "colab_type": "text"
      },
      "source": [
        "Apply the Scaling Methods for Amount and Time Columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbrZd3zZIdIy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Code Here\n",
        "#Apply the Standrad Scaler for Amount and RobustScaler for Time, Robustscaler less prone to Outliers\n",
        "'''example:\n",
        "ccfraud['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\n",
        "ccfraud['scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpMX6TcvRWT6",
        "colab_type": "text"
      },
      "source": [
        "By appling the Scaling, we got 2 new columns namely scaled_amount,scaled_time,Drop the Time and Amount columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoxXls3wRg39",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Code Here\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPIyPJVtLwaO",
        "colab_type": "text"
      },
      "source": [
        "**Splitting the Data (Original DataFrame)**\n",
        "Before proceeding with the Random UnderSampling technique we have to separate the orginal dataframe. Why? for testing purposes, remember although we are splitting the data when implementing Random UnderSampling or OverSampling techniques, we want to test our models on the original testing set not on the testing set created by either of these techniques. The main goal is to fit the model either with the dataframes that were undersample and oversample (in order for our models to detect the patterns), and test it on the original testing set.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wy5lS2YVSKKV",
        "colab_type": "text"
      },
      "source": [
        "Deinfe the Feature Representation and Class Columns (X & Y)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cZL01tSL22r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#X is all the columns except class\n",
        "#y is Class columns\n",
        "#Code Here \n",
        "\n",
        "X =\n",
        "\n",
        "y ="
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSvoskOjSXOh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Split the X and Y with train and test data using StratifiedKFold Cross validation technique\n",
        "#Code here\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoI-IX8PL2cH",
        "colab_type": "text"
      },
      "source": [
        "**Imbalance Dataset handling using SMOTE Technique**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QahD4jVAUFGK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Code here\n",
        "\n",
        "'''\n",
        "example:\n",
        "\n",
        "print(sum(y_train==1))\n",
        "print(sum(y_train==0))\n",
        "sm = SMOTE(random_state=2)\n",
        "X_train_res, y_train_res = sm.fit_sample(X_train, y_train.ravel())\n",
        "\n",
        "print(sum(y_train_res==1))\n",
        "print(sum(y_train_res==0))\n",
        "\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5Lus1NSS-Rk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# See if both the train and test label distribution are similarly distributed\n",
        "\n",
        "'''\n",
        "example:\n",
        "\n",
        "train_unique_label, train_counts_label = np.unique(y_train, return_counts=True)\n",
        "test_unique_label, test_counts_label = np.unique(y_test, return_counts=True)\n",
        "print('-' * 100)\n",
        "\n",
        "print('Label Distributions: \\n')\n",
        "print(train_counts_label/ len(y_train))\n",
        "print(test_counts_label/ len(y-_est))\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LCTmuzgU3JZ",
        "colab_type": "text"
      },
      "source": [
        "Equally Distributing and Corelating"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNfvPepKU2cU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Code here\n",
        "'''Example:\n",
        "print('Distribution of the Classes in the subsample dataset')\n",
        "print(new_df['Class'].value_counts()/len(new_df))\n",
        "\n",
        "\n",
        "\n",
        "sns.countplot('Class', data=new_df, palette=colors)\n",
        "plt.title('Equally Distributed Classes', fontsize=14)\n",
        "plt.show()\n",
        "\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}